{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install gradio gtts","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gradio as gr\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom gtts import gTTS\nfrom huggingface_hub import login\n\n# -------------------------\n# Authenticate using the Hugging Face token\n# -------------------------\nlogin(token=\"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\")\n\n# -------------------------\n# 1. Translation Setup\n# -------------------------\n# Updated and corrected translation models\ntranslation_models = {\n    \"fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n    \"de\": \"Helsinki-NLP/opus-mt-en-de\",\n    \"ja\": \"Helsinki-NLP/opus-mt-en-jap\",\n    \"hi\": \"Helsinki-NLP/opus-mt-en-hi\",\n    \"es\": \"Helsinki-NLP/opus-mt-en-es\",\n    \"ru\": \"Helsinki-NLP/opus-mt-en-ru\",\n    \"zh\": \"Helsinki-NLP/opus-mt-en-zh\",\n    \"it\": \"Helsinki-NLP/opus-mt-en-it\",\n    \"pt\": \"Helsinki-NLP/opus-mt-tc-big-en-pt\",\n    \"ko\": \"Helsinki-NLP/opus-mt-tc-big-en-ko\",  # Updated working model for Korean\n    \"tr\": \"Helsinki-NLP/opus-mt-en-trk\",\n    \"id\": \"Helsinki-NLP/opus-mt-en-id\",\n    \"el\": \"Helsinki-NLP/opus-mt-en-el\",\n    \"nl\": \"Helsinki-NLP/opus-mt-en-nl\",\n    \"sv\": \"Helsinki-NLP/opus-mt-en-sv\",\n    \"pl\": \"allegro/BiDi-eng-pol\",\n    \"th\": \"facebook/m2m100_418M\",\n    \"ar\": \"Helsinki-NLP/opus-mt-en-ar\",\n    \"fi\": \"Helsinki-NLP/opus-mt-en-fi\",\n    \"cs\": \"Helsinki-NLP/opus-mt-en-cs\",\n    \"ro\": \"Helsinki-NLP/opus-mt-en-ro\",\n    \"hu\": \"Helsinki-NLP/opus-mt-en-hu\",\n    \"no\": \"Helsinki-NLP/opus-mt-en-no\",\n    \"da\": \"Helsinki-NLP/opus-mt-en-da\",\n}\n\n\ndef get_translation_model(target_lang_code):\n    model_name = translation_models.get(target_lang_code)\n    if not model_name:\n        raise ValueError(f\"No translation model found for language code: {target_lang_code}\")\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    return tokenizer, model\n\n\ndef translate_text(text, target_lang_code):\n    tokenizer, model = get_translation_model(target_lang_code)\n    tokens = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n    translation = model.generate(**tokens)\n    return tokenizer.decode(translation[0], skip_special_tokens=True)\n\n# -------------------------\n# 2. Story Generation\n# -------------------------\ndef generate_story(country, language):\n    input_prompt = f\"\"\"\n    You just arrived in {country} and step out at a famous landmark.\n    As you're looking around, a friendly local greets you in {language} and begins to share two things:\n\n    1. A unique greeting tradition in their culture ‚Äî including a short dialogue in {language} (with English translation).\n    2. A traditional practice ‚Äî such as how they eat, what they wear, or a daily ritual ‚Äî again with a short {language} dialogue and its English explanation.\n\n    Keep the tone fun, friendly, and natural. Make the story engaging as if you're truly experiencing the moment.\n    The local should say something like \"Ciao! Benvenuto in Italia!\" then explain what it means and why they say it.\n    \"\"\"\n\n    model_name = \"gpt2-medium\"\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n\n    inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n    outputs = model.generate(\n        **inputs,\n        max_length=500,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return story.strip()\n\n# -------------------------\n# 3. Text-to-Speech\n# -------------------------\ndef narrate_story(text, lang_code=\"en\"):\n    tts = gTTS(text, lang=lang_code)\n    audio_path = \"story.mp3\"\n    tts.save(audio_path)\n    return audio_path\n\n# -------------------------\n# 4. Main Pipeline\n# -------------------------\ndef generate_and_translate(country, language, lang_code):\n    story = generate_story(country, language)\n    translated = translate_text(story, lang_code)\n    audio = narrate_story(translated, lang_code)\n    return story, translated, audio\n\n# -------------------------\n# 5. Gradio Interface\n# -------------------------\ncountries = {\n    \"France\": \"fr\",\n    \"Germany\": \"de\",\n    \"Japan\": \"ja\",\n    \"India\": \"hi\",\n    \"Spain\": \"es\",\n    \"Russia\": \"ru\",\n    \"China\": \"zh\",\n    \"Italy\": \"it\",\n    \"Brazil\": \"pt\",\n    \"South Korea\": \"ko\",\n    \"Turkey\": \"tr\",\n    \"Indonesia\": \"id\",\n    \"Greece\": \"el\",\n    \"Netherlands\": \"nl\",\n    \"Sweden\": \"sv\",\n    \"Poland\": \"pl\",\n    \"Thailand\": \"th\"\n}\n\ndef app_flow(selected_country):\n    lang_code = countries[selected_country]\n    story, translated, audio = generate_and_translate(selected_country, selected_country, lang_code)\n    return story, translated, audio\n\niface = gr.Interface(\n    fn=app_flow,\n    inputs=gr.Dropdown(choices=list(countries.keys()), label=\"Select a Country\"),\n    outputs=[\n        gr.Textbox(label=\"Original Story (English)\"),\n        gr.Textbox(label=\"Translated Story\"),\n        gr.Audio(label=\"Narrated Story\")\n    ],\n    title=\"üåç GlobeTales: Learn Language Through Stories\",\n    description=\"Choose a country to hear a fun story narrated in its language with cultural facts and vocabulary!\"\n)\n\niface.launch(debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:12:01.680238Z","iopub.execute_input":"2025-04-25T15:12:01.680513Z","iopub.status.idle":"2025-04-25T15:14:13.599691Z","shell.execute_reply.started":"2025-04-25T15:12:01.680491Z","shell.execute_reply":"2025-04-25T15:14:13.599118Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://369480889255a98580.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://369480889255a98580.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9e94bc8003540b9abcba2c4168e3d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6bbb005572544e6b2a08b965b945688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67653a954c914d2db699329dd11c82e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf9ff3c931040318aadba69289a6523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23df7e66654f41bb902a309b6ee5e56e"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc90a3d458c4a85b2debd8413fae17a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f38d4b768d41d2bbf304cc0357f58b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea39a912ea2418b982a7159e8f623d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/816k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b6a2f201644e27a78b2f25a8b955a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/804k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4fcc96555a4a44878ee02e81e009ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/416 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e395801c214115befc43888264fb22"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae289f0a95c41efa113e22431b843bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/837M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8905033c7f0c48298d5d602a1104bb86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff2c90aac6545f7a9c5272f7c48119d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4106: FutureWarning: \n`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n`__call__` method to prepare your inputs and targets.\n\nHere is a short example:\n\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n\nIf you either need to use different keyword arguments for the source and target texts, you should do two calls like\nthis:\n\nmodel_inputs = tokenizer(src_texts, ...)\nlabels = tokenizer(text_target=tgt_texts, ...)\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\n\nSee the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\nFor a more complete example, see the implementation of `prepare_seq2seq_batch`.\n\n  warnings.warn(formatted_warning, FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://369480889255a98580.gradio.live\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}